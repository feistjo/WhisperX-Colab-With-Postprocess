{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iFcTgQvYKCAR"
      },
      "source": [
        "This is for the first run only - for future runs, you can skip to \"Run the code block to connect Google Drive.\"\n",
        "\n",
        "First, prepare your Hugging Face token to access required models for diarization. You can generate the token [here](https://huggingface.co/settings/tokens), making it a \"Read\" token, and paste it in a text document, since you'll need it later.\n",
        "\n",
        "You then need to accept the user agreement for the following models: [Segmentation](https://huggingface.co/pyannote/segmentation-3.0) and [Speaker-Diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)\n",
        "\n",
        "To add the token to your Colab secrets, click the key on the left of this screen, click \"Add new secret\", name it \"HF_TOKEN\", paste the token into Value, and make sure Notebook access is on."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "35d1Q6ljohfF"
      },
      "source": [
        "Run the code block to connect Google Drive to get file(s) to transcribe and save transcriptions. Make sure the source file(s) (mp4 and wav are supported) are in a folder labeled \"transcription\" in the root folder of your Drive (\"My Drive\"). You will get multiple popups asking if you want to connect and are sure, give it access. You will also get a popup saying this code was not written by Google, click run anyway.\n",
        "\n",
        "To run each code block, click the play button in the top left, then wait for it to finish.\n",
        "\n",
        "Note: All files in \"My Drive/transciption\" will be processed, so make sure to move the old files out of this folder before continuing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYMOMNCXRdUM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iTMWCue-oX11"
      },
      "source": [
        "Run to set up the environment, you will get a prompt to restart session. Wait for it to finish before restarting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyDDfHx7C3yW"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch==2.0.0 torchaudio=2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "!pip install git+https://github.com/m-bain/whisperx.git\n",
        "!pip install ctranslate2==4.4.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hgvcmo7NpHEU"
      },
      "source": [
        "Run to do the actual transcription, alignment, and diarization with WhisperX, outputting a not-quite human readable format and a version meant to be more human readable. This is adapted from the example at https://github.com/m-bain/whisperX This takes about 15 minutes on a 2h21m vidoe pre-converted to .wav. This will print a message when finished."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlgqSBVXE_rw"
      },
      "outputs": [],
      "source": [
        "import whisperx\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "from google.colab import userdata\n",
        "if not userdata.get('HF_TOKEN'):\n",
        "  print(\"Error: missing HF_TOKEN! Please go to https://huggingface.co/settings/tokens to generate a token and add it to your notebook secrets\")\n",
        "\n",
        "device = \"cuda\"\n",
        "batch_size = 16 # reduce if low on GPU mem\n",
        "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "\n",
        "# 1. Transcribe with original whisper (batched)\n",
        "model = whisperx.load_model(\"large-v3\", device, compute_type=compute_type)\n",
        "\n",
        "def transcribe(audio_file_name):\n",
        "  audio = whisperx.load_audio(audio_file_name)\n",
        "  result = model.transcribe(audio, batch_size=batch_size)\n",
        "  #print(result[\"segments\"]) # before alignment\n",
        "\n",
        "  # delete model if low on GPU resources\n",
        "  # import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
        "\n",
        "  # 2. Align whisper output\n",
        "  model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "  result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "\n",
        "  #print(result[\"segments\"]) # after alignment\n",
        "\n",
        "  # delete model if low on GPU resources\n",
        "  # import gc; gc.collect(); torch.cuda.empty_cache(); del model_a\n",
        "\n",
        "  # 3. Assign speaker labels\n",
        "  diarize_model = whisperx.DiarizationPipeline(use_auth_token=userdata.get('HF_TOKEN'), device=device)\n",
        "\n",
        "  # add min/max number of speakers if known\n",
        "  diarize_segments = diarize_model(audio_file_name)\n",
        "  # diarize_model(audio_file, min_speakers=min_speakers, max_speakers=max_speakers)\n",
        "\n",
        "  result = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "  #print(diarize_segments)\n",
        "  #print(result[\"segments\"]) # segments are now assigned speaker IDs\n",
        "  with open(str(audio_file_name) + \".txt\", \"w\") as f :\n",
        "      f.write(str(result[\"segments\"]))\n",
        "\n",
        "import glob, os\n",
        "files = glob.glob(\"/content/drive/MyDrive/transcription/*.*\")\n",
        "print(\"Files to transcribe:\")\n",
        "for i in range(len(files)):\n",
        "  if (os.path.isfile(files[i]) and (files[i].endswith(\".wav\") or files[i].endswith(\".mp4\"))):\n",
        "    print(files[i])\n",
        "\n",
        "for i in range(len(files)):\n",
        "  if (os.path.isfile(files[i]) and files[i].endswith(\".wav\")):\n",
        "    print(\"Transcribing file \" + files[i])\n",
        "    transcribe(files[i])\n",
        "  elif (os.path.isfile(files[i]) and (files[i].endswith(\".mp4\") or files[i].endswith(\"mp3\"))):\n",
        "    print(\"Converting and transcribing file \" + files[i])\n",
        "    basefile = os.path.splitext(files[i])[0]\n",
        "    #convert to wav\n",
        "    !ffmpeg -i \"{files[i]}\" \"{basefile}.wav\"\n",
        "    transcribe(basefile + \".wav\")\n",
        "    os.remove(basefile + \".wav\")\n",
        "\n",
        "\n",
        "from ast import literal_eval\n",
        "import sys\n",
        "import math\n",
        "import os\n",
        "\n",
        "\n",
        "def seconds_to_h_m_s(seconds):\n",
        "    return (\n",
        "        str(math.floor(seconds / 3600))\n",
        "        + \":\"\n",
        "        + str(math.floor((seconds % 3600) / 60))\n",
        "        + \":\"\n",
        "        + \"{:.2f}\".format(seconds % 60)\n",
        "    )\n",
        "\n",
        "\n",
        "def to_string(data):\n",
        "    current_speaker = None\n",
        "    string = \"\"\n",
        "    while len(data) > 0:\n",
        "        if not \"speaker\" in data[0]:\n",
        "            data[0][\"speaker\"] = \"SPEAKER_UNK\"\n",
        "\n",
        "        if current_speaker is None or current_speaker != data[0][\"speaker\"]:\n",
        "            current_speaker = data[0][\"speaker\"]\n",
        "            string += (\n",
        "                current_speaker\n",
        "                + \":\\n\\t\"\n",
        "                + seconds_to_h_m_s(data[0][\"start\"])\n",
        "                + \": \"\n",
        "                + data[0][\"text\"]\n",
        "                + \"\\n\"\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            string += (\n",
        "                \"\\t\"\n",
        "                + seconds_to_h_m_s(data[0][\"start\"])\n",
        "                + \": \"\n",
        "                + data[0][\"text\"]\n",
        "                + \"\\n\"\n",
        "            )\n",
        "        data = data[1:]\n",
        "    return string\n",
        "\n",
        "def make_readable(filename):\n",
        "  # Open the JSON file for reading\n",
        "  with open(filename, \"r\") as f:\n",
        "      # Load the JSON file into a variable\n",
        "      data = literal_eval(f.read())\n",
        "      # print(data[0][\"text\"])\n",
        "      # print(data[0][\"speaker\"])\n",
        "      sys.setrecursionlimit(3000)\n",
        "      with open(os.path.splitext(filename)[0] + \"_readable.txt\", \"w\") as f:\n",
        "          #print(to_string(data))\n",
        "          f.write(to_string(data))\n",
        "\n",
        "import glob, os\n",
        "files = glob.glob(\"/content/drive/MyDrive/transcription/*.txt\")\n",
        "for i in range(len(files)):\n",
        "  if (os.path.isfile(files[i]) and files[i].endswith(\".txt\") and not files[i].endswith(\"_readable.txt\")):\n",
        "    print(\"Making \" + files[i] + \" readable \")\n",
        "    make_readable(files[i])\n",
        "\n",
        "print(\"Done! Look in the transcription folder for the results\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
